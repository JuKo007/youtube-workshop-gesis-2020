<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Automatic sampling and analysis of YouTubeData</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Kohne Johannes Breuer M. Rohangis Mohseni" />
    <meta name="date" content="2020-02-10" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="../workshop.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic sampling and analysis of YouTubeData
## The YouTube API
### Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni
### 2020-02-10

---


class: center, middle

# Overview
 
---

# Overview

- All data on YouTube is stored in a [MySQL](https://en.wikipedia.org/wiki/MySQL) database
 
- The website itself is an HTML page, which loads content from this database

- The HTML is rendered by a webbrowser so the user can interact with it

- Through interacting with the rendered website, we can either retrieve content from the database
or send content to the database

- The Youtube Website is
    - build in [HTML](https://de.wikipedia.org/wiki/Hypertext_Markup_Language), 
    - uses [CSS](https://de.wikipedia.org/wiki/Cascading_Style_Sheets) for the "styling"
    - dynamically loads content using [Ajax](https://en.wikipedia.org/wiki/Ajax_(programming) ) from the Database

![plot](Images/YouTube_better.png)
---
# How do we get data from Websites?

- Theoretically, we could gather all the information manually by clicking on the things that are
 interesting to us and copy/pasting them. However, this is tedious and time-consuming.**We want a way
 of automatizing this task**

- In general, there are two ways of getting data from a Website ( [Webscraping](https://en.wikipedia.org/wiki/Web_scraping))

 1)  **Screenscraping:** Getting the HTML-code out of your browser, parsing &amp; formatting it, then analyzing the data
  
 2)  **API-harvesting:** Sending requests directly to the database and only getting back the information that you want and need.

---
class: center, middle

# Screenscraping

---
# Screenscraping

- Screenscraping means that we are downloading the HTML textfile, which contains the content we are interested in but also a lot of unnecessary clutter that describes how the website should be rendered by the browser

![plot](Images/Screenscraping_2.png)
---
# Screenscraping

![plot](Images/BBC2.png)
---
# Screenscraping: GETting Data

- To automatically obtain data, we can use a so called [GET      request](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol)

- A GET request is an HTTP method for asking a server to send a specific resource (usually an HTML page) back
to you local machine

- You can try it out in your console with ```{r}GET https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol ```

- This is the basic principle that all the Scraping packages build-on.

- We will not use it directly and will let the higher-level applications handle this under the hood

---
---
font-size: x-small

# Screenscraping with Rvest

- Lets try to get this information using screenscraping from R


```r
# installing packages and attaching package
if ("rvest" %in% installed.packages() != TRUE) {
  install.packages("rvest")};library(rvest)
## Loading required package: xml2

# defining website and Xpath from inspect function in browser
page &lt;- "https://www.youtube.com/watch?v=4PqdqWWSHJY"
Xp &lt;- "/html/body/div[2]/div[4]/div/div[5]/div[2]/div[2]/div/div[2]/meta[2]"

# getting page
Website &lt;- read_html(page)

# getting node containing the description
Description &lt;- html_nodes(Website, xpath = Xp)

# printing description
html_attr(Description, name = "content")
## [1] "The Conservatives and Labour have been outlining their main pitch to voters. The Prime Minister Boris Johson in his first major speech of the campaign said a..."
```
---
# Screenscraping

- **Problem:** We only get part of the decsription, the text seems to cut off at a certain point

- This happens because on Youtube, you have to click on "show more" to see the full description. Youtube is a **dynamic** website that loads new content based on what users do on the website. With each action, the HTML of the
site changes without having to reload
    - More comments only load when you scroll down
    - You have to click on "show more" to see the video description
    - you have to click on "show answers" to see more comment replies
  
- This makes screenscraping **a lot** harder

- Most social media websites and websites with multimedia content are dynamic

![plot](Images/YouTube_better.png)

---
# Screenscraping with RSelenium


```r
# We first have to configure docker and open a docker container:
# https://callumgwtaylor.github.io/blog/2018/02/01/using-rselenium-and-docker-to-webscrape-in-r-using-the-who-snake-database/

# installing packages
if ("RSelenium" %in% installed.packages() != TRUE) {
  install.packages("RSelenium")
}

# opening docker container from system
check &lt;- system2("docker", args = "ps", stdout = TRUE)

if (length(check) &gt; 1) {
  
  #start new container
  system2("docker", args = c("run", "-d", "-p", "4445:4444", "selenium/standalone-chrome"))
  
} else {
  
  # kill old container
  DockerName &lt;- trimws(strsplit(check[2],"tcp")[[1]][2])
  system2("docker", args = c("kill",DockerName))
  
  # start new container
  system2("docker", args = c("run", "-d", "-p", "4445:4444", "selenium/standalone-chrome"))
  
}

# attaching packages
library(RSelenium)

# Assigning google chrome docker session
remDr &lt;- RSelenium::remoteDriver(remoteServerAddr = "localhost",
                                 port = 4445L,
                                 browserName = "chrome")
```

---
# Screenscraping with RSelenium

- We can then navigate to the website and print a screenshot


```r
# Open remote connection
remDr$open()
```

```
## [1] "Connecting to remote server"
## $acceptInsecureCerts
## [1] FALSE
## 
## $browserName
## [1] "chrome"
## 
## $browserVersion
## [1] "77.0.3865.75"
## 
## $chrome
## $chrome$chromedriverVersion
## [1] "77.0.3865.40 (f484704e052e0b556f8030b65b953dce96503217-refs/branch-heads/3865@{#442})"
## 
## $chrome$userDataDir
## [1] "/tmp/.com.google.Chrome.jl3m0f"
## 
## 
## $`goog:chromeOptions`
## $`goog:chromeOptions`$debuggerAddress
## [1] "localhost:40979"
## 
## 
## $networkConnectionEnabled
## [1] FALSE
## 
## $pageLoadStrategy
## [1] "normal"
## 
## $platformName
## [1] "linux"
## 
## $proxy
## named list()
## 
## $setWindowRect
## [1] TRUE
## 
## $strictFileInteractability
## [1] FALSE
## 
## $timeouts
## $timeouts$implicit
## [1] 0
## 
## $timeouts$pageLoad
## [1] 300000
## 
## $timeouts$script
## [1] 30000
## 
## 
## $unhandledPromptBehavior
## [1] "dismiss and notify"
## 
## $webdriver.remote.sessionid
## [1] "be4f24b179ec84bed3d7f9d57f74e745"
## 
## $id
## [1] "be4f24b179ec84bed3d7f9d57f74e745"
```

```r
# Navigate to website
remDr$navigate("https://www.youtube.com/watch?v=4PqdqWWSHJY")

# take screenshot
remDr$screenshot(display = TRUE)

# saving Screenshot
remDr$screenshot(file = 'Images/RSeleniumScreenshot.png')
```

---
# Screenshot 

![plot](Images/RSeleniumScreenshot.png)

---
# Screenscraping with RSelenium

 - We can then navigate to the "show more" button, and click it


```r
# Xpath of "show more" button (using inspect element in Browser)
xp &lt;- '//*[@id="more"]/yt-formatted-string'

# navigating to button element
element &lt;- remDr$findElement(using = 'xpath', xp)

# click on button
element$clickElement()

# take screenshot (we can see that the description box is now expanded)
remDr$screenshot(display = TRUE, useViewer = TRUE)

#save Screenshot
remDr$screenshot(file = 'Images/RSeleniumScreenshot2.png')
```
---
# Screenshot 

![plot](Images/RSeleniumScreenshot2.png)

---
# Screenscraping with RSelenium

- We can then extract the contents of the expanded description box


```r
#navigating to description element
xp2 &lt;- '//*[@id="description"]/yt-formatted-string'
element2 &lt;- remDr$findElement(using = 'xpath', xp2)

# get element text
unlist(element2$getElementText())
```

```
## [1] "The Conservatives and Labour have been outlining their main pitch to voters. The Prime Minister Boris Johson in his first major speech of the campaign said a Conservative government would unite the country and \"level up\" the prospects for people with massive investment in health, better infrastructure, more police, and a green revolution. But he said the key issue to solve was Brexit. Meanwhile Labour vowed to outspend the Tories on the NHS in England. \n\nLabour leader Jeremy Corbyn has also faced questions over his position on allowing a second referendum on Scottish independence. Today at the start of a two-day tour of Scotland, he said wouldn't allow one in the first term of a Labour government but later rowed back saying it wouldn't be a priority in the early years. \n\nSophie Raworth presents tonight's BBC News at Ten and unravels the day's events with the BBC's political editor Laura Kuenssberg, health editor Hugh Pym and Scotland editor Sarah Smith.\n\n\nPlease subscribe HERE http://bit.ly/1rbfUog"
```

- "Jeremy Corbyn has launched Labour's general election manifesto and vowed to transform the United Kingdom if they win. He unveiled the wide ranging plans in Birmingham promising what he called a 'green transformation of the economy and a 'manifesto for hope'.   There'd be big tax increases on higher earners and companies to pay for the plans.  The BBC's Political Editor, Laura Kuenssberg, Political Correspondent Alex Forsyth and Economics Editor Faisal Islam report. \nPlease subscribe HERE http://bit.ly/1rbfUog"

---
# ScreenScraping

- Advantages of Screenscraping:
  + You can access everything that you are able to access from your browser
  + You are (theoretically) not restricted in how much data you can get
  + (Theoretically) Independent from API-restrictions

- Disadvantages of Screenscraping:
  - Extremely tedious to get information out of HTML-pages
  - You have to manually look up the Xpaths/CSS/HTML containers to get specific information
  - Reproducibility: The website might be tailored to stuff in your Cache, Cookies, Accounts etc.
  - There is no guarantee that even pages that look the same have the same underlying HTML structure
  - You have to manually check the website and your data to make sure that you're getting what you want
  - If the website changes anything in their styling, your scripts won't work anymore
  - Legal Gray Area (recent [court ruling](http://cdn.ca9.uscourts.gov/datastore/opinions/2019/09/09/17-16783.pdf) though)

---
class: center, middle

# API-Harvesting

---

# API-harvesting

- An [**A**pplication **P**rogramming **I**nterface]([API](https://en.wikipedia.org/wiki/Application_programming_interface)) is:
  - system build for developers
  - directly communicate with the database
  - Voluntary service of the website
  - they can dictate what information is accessible, to whom, how, and in which quantities.

![plot](Images/API_harvesting.png)

---
# API-harvesting

- APIs can be used to:

  - embed content in other applications
  - create Bots that do something automatically
  - scheduling/moderation for content creators
  - collect data for (market) research purposes

- Not every website has their own API. However, most large Social Media Websites do
  - [Facebook](https://developers.facebook.com/docs/graph-api?locale=de_DE)
  - [Twitter](https://developer.twitter.com/en/docs/basics/getting-started)
  - [Instagram](https://www.instagram.com/developer/)
  - [Wikipedia](https://de.wikipedia.org/wiki/Wikipedia:Technik/Datenbank/API)
  - [Google Maps](https://www.programmableweb.com/api/google-maps-places)

---

# API-harvesting

- Advantages of API-harvesting:
  + No need to interact with HTML files, you only get the information you asked for
  + The data you get is already nicely formatted (usually JSON files)
  + You can be sure that what you do is perfectly legal and in line with ToS

- Disadvantages of API-harvesting:
  - Not every website has an API
  - You can only get what the company allows you to get
  - There are often restricting quotas (querys per day)
  - there is no standard language to make querys, you have to check the documentation
  - Not every API has a documentation
  
---
class: inverse, middle, center

# Screenscraping vs. API-harvesting

If you can, use an API, if you must, use Screenscraping instead

---
class: center, middle

# The YouTube API

---

# Summary

- Fortunately, YouTube has their own, well-documented API
  that developers can use to interact with their database (Most Google Services do)

- To find an API for a given website, [Programmable Web](https://www.programmableweb.com) is
  a good starting point

- Some prominent APIs:
  - [Google Maps](https://www.programmableweb.com/api/google-maps-places)
  - [Facebook](https://developers.facebook.com/docs/graph-api?locale=de_DE)
  - [Twitter](https://developer.twitter.com/en/docs/basics/getting-started)
  - [Instagram](https://www.instagram.com/developer/)
  - [Wikipedia](https://de.wikipedia.org/wiki/Wikipedia:Technik/Datenbank/API)

- We will use the [YouTube API](https://developers.google.com/youtube/v3/docs) today

---

# Getting API Access

- First of all, we need a Google account.
  - You can use an already existing one
  - you can create a new one
  - For safety reasons, we advise you to create a new account
    (If you accidentally share your login credentials, people using the credentials only
     have access to your Bot account and not your personal or work account)
  - You can create a new account [here](https://accounts.google.com/signup/v2/webcreateaccount?service=mail&amp;continue=https%3A%2F%2Fmail.google.com%2Fmail%2F&amp;ltmpl=googlemail&amp;gmb=exp&amp;biz=false&amp;flowName=GlifWebSignIn&amp;flowEntry=SignUp)

- Second, we need to:
  - Sign up for the Google developer console
  - Create a new Project
  - Activate the YouTube Data API
  - Create Authentification Credentials

---
# Google Developer Console

- Go to the [Developer Console](https://console.developers.google.com) and log in with your (new) Google Account

- Create a new Project by clicking on the "Create" button on the top right Corner

![plot](Images/CreateProject.png)

---
# Watch out

- You only have a limited number of Projects that you can create, so be carefull not
  to constantly delete and create new ones

- More information can be found [here](https://support.google.com/cloud/answer/6330231)

---
# Creating a New Project

- Specify a Project Name (you do not have to indicate an organisation) and click on create once you are done

![plot](Images/ProjectName.png)
---
# Activate the YouTube Data v3 API

- click on the button to "Activate APIs"

![plot](Images/ActivateAPI.png)

---
# Activate the YouTube Data and Analytics APIs

- Enter "YouTube" in the search bar, click on the data API and then activate it by clicking on the "Activate" button

![plot](Images/APIs.png)

---

![plot](Images/Activate.png)

---

![plot](Images/AuthentificationButton.png)

---

![plot](Images/UserAgreement.png)

---

![plot](Images/CreateOAuth.png)

---

![plot](Images/OtherApplication.png)

---
class: center, middle

# DONE!

---
# What can we do with these Credentials now?

- We can see what the API allows us to do in the [Google API Explorer](https://developers.google.com/apis-explorer/#p/)

- Basically, you can automate any action and see all data that you
  also would be able to see when logged into YT with your Google account

- Fortunately for us, the YouTube API is very [well documented](https://developers.google.com/youtube/v3/docs/)

---

# Test

---

# Have a slide with an overview of most common parameters and which ones we are going to use

---

# Have a slide explaining what a GET Request it, how to execute it from the terminal and in the toolkit

---

# how to execute it from R

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
