---
title: "Automatic sampling and analysis of YouTubeData"
subtitle: "The YouTube API"
author: "Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni"
date: "2020-02-10"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---

class: center, middle

# Overview
 
---

# Overview

- All data on YouTube is stored in a [MySQL](https://en.wikipedia.org/wiki/MySQL) database
 
- The website itself is an HTML page, which loads content from this database

- The HTML is rendered by a webbrowser so the user can interact with it

- Through interacting with the rendered website, we can either retrieve content from the database
or send content to the database

- The Youtube Website is
    - build in [HTML](https://de.wikipedia.org/wiki/Hypertext_Markup_Language), 
    - uses [CSS](https://de.wikipedia.org/wiki/Cascading_Style_Sheets) for the "styling"
    - dynamically loads content using [Ajax](https://en.wikipedia.org/wiki/Ajax_(programming) ) from the Database

![plot](Images/YouTube_better.png)
---

# How do we get data from Websites?

- Theoretically, we could gather all the information manually by clicking on the things that are
 interesting to us and copy/pasting them. However, this is tedious and time-consuming.**We want a way
 of automatizing this task**

- In general, there are two ways of getting data from a Website ( [Webscraping](https://en.wikipedia.org/wiki/Web_scraping))

 1)  **Screenscraping:** Getting the HTML-code out of your browser, parsing & formatting it, then analyzing the data
  
 2)  **API-harvesting:** Sending requests directly to the database and only getting back the information that you want and need.

---

class: center, middle

# Screenscraping

---

# Screenscraping

- Screenscraping means that we are downloading the HTML textfile, which contains the content we are interested in but also a lot of unnecessary clutter that describes how the website should be rendered by the browser

![plot](Images/Screenscraping_2.png)
---
# Screenscraping

![plot](Images/BBC2.png)
---

font-size: x-small

# Screenscraping with Rvest

- Lets try to get this information using screenscraping from R

```{r collapse = TRUE}
# installing packages and attaching package
if ("rvest" %in% installed.packages() != TRUE) {
  install.packages("rvest")};library(rvest)

# defining website and Xpath from inspect function in browser
page <- "https://www.youtube.com/watch?v=4PqdqWWSHJY"
Xp <- "/html/body/div[2]/div[4]/div/div[5]/div[2]/div[2]/div/div[2]/meta[2]"

# getting page
Website <- read_html(page)

# getting node containing the description
Description <- html_nodes(Website, xpath = Xp)

# printing description
html_attr(Description, name = "content")
```
---

# Screenscraping

- **Problem:** We only get part of the decsription, the text seems to cut off at a certain point

- This happens because on Youtube, you have to click on "show more" to see the full description. Youtube is a **dynamic** website that loads new content based on what users do on the website. With each action, the HTML of the
site changes without having to reload
    - More comments only load when you scroll down
    - You have to click on "show more" to see the video description
    - you have to click on "show answers" to see more comment replies
  
- This makes screenscraping **a lot** harder

- Most social media websites and websites with multimedia content are dynamic

![plot](Images/YouTube_better.png)

---

# Screenscraping with RSelenium

```{r}

# We first have to configure docker and open a docker container:
# https://callumgwtaylor.github.io/blog/2018/02/01/using-rselenium-and-docker-to-webscrape-in-r-using-the-who-snake-database/

# installing packages
if ("RSelenium" %in% installed.packages() != TRUE) {
  install.packages("RSelenium")
}

# opening docker container from system
check <- system2("docker", args = "ps", stdout = TRUE)

if (length(check) > 1) {
  
  #start new container
  system2("docker", args = c("run", "-d", "-p", "4445:4444", "selenium/standalone-chrome"))
  
} else {
  
  # kill old container
  DockerName <- trimws(strsplit(check[2],"tcp")[[1]][2])
  system2("docker", args = c("kill",DockerName))
  
  # start new container
  system2("docker", args = c("run", "-d", "-p", "4445:4444", "selenium/standalone-chrome"))
  
}

# attaching packages
library(RSelenium)

# Assigning google chrome docker session
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",
                                 port = 4445L,
                                 browserName = "chrome")
```

---

# Screenscraping with RSelenium

- We can then navigate to the website and print a screenshot

```{r}

# Open remote connection
remDr$open()

# Navigate to website
remDr$navigate("https://www.youtube.com/watch?v=4PqdqWWSHJY")

# take screenshot
remDr$screenshot(display = TRUE)
```

---

# Insert Screenshot taken with rmDr here

---

# Screenscraping with RSelenium

 - We can then navigate to the "show more" button, and click it

```{r results='hide'}

# Xpath of "show more" button (using inspect element in Browser)
xp <- '//*[@id="more"]/yt-formatted-string'

# navigating to button element
element <- remDr$findElement(using = 'xpath', xp)

# click on button
element$clickElement()

# take screenshot (we can see that the description box is now expanded)
remDr$screenshot(display = TRUE, useViewer = TRUE)
```
---

# Insert Screenshot taken with rmDr here

---

# Screenscraping with RSelenium

- We can then extract the contents of the expanded description box

```{r}

#navigating to description element
xp2 <- '//*[@id="description"]/yt-formatted-string'
element2 <- remDr$findElement(using = 'xpath', xp2)

# get element text
unlist(element2$getElementText())

```

- INSERT TEXT CHUNK HERE

---

# ScreenScraping

- Advantages of Screenscraping:
  + You can access everything that you are able to access from your browser
  + You are (theoretically) not restricted in how much data you can get
  + (Theoretically) Independent from API-restrictions

- Disadvantages of Screenscraping:
  - Extremely tedious to get information out of HTML-pages
  - You have to manually look up the Xpaths/CSS/HTML containers to get specific information
  - Reproducibility: The website might be tailored to stuff in your Cache, Cookies, Accounts etc.
  - There is no guarantee that even pages that look the same have the same underlying HTML structure
  - You have to manually check the website and your data to make sure that you're getting what you want
  - If the website changes anything in their styling, your scripts won't work anymore
  - Legal Gray Area (recent [court ruling](http://cdn.ca9.uscourts.gov/datastore/opinions/2019/09/09/17-16783.pdf) though)

---

class: center, middle

# API-Harvesting

---

# API-harvesting

- An [**A**pplication **P**rogramming **I**nterface]([API](https://en.wikipedia.org/wiki/Application_programming_interface)) is:
  - system build for developers
  - directly communicate with the database
  - Voluntary service of the website
  - they can dictate what information is accessible, to whom, how, and in which quantities.

![plot](Images/API_harvesting.png)

---

# API-harvesting

- APIs can be used to:

  - embed content in other applications
  - create Bots that do something automatically
  - scheduling/moderation for content creators
  - collect data for (market) research purposes

- Not every website has their own API. However, most large Social Media Websites do
  - [Facebook](https://developers.facebook.com/docs/graph-api?locale=de_DE)
  - [Twitter](https://developer.twitter.com/en/docs/basics/getting-started)
  - [Instagram](https://www.instagram.com/developer/)
  - [Wikipedia](https://de.wikipedia.org/wiki/Wikipedia:Technik/Datenbank/API)
  - [Google Maps](https://www.programmableweb.com/api/google-maps-places)

---

# API-harvesting

- Advantages of API-harvesting:
  + No need to interact with HTML files, you only get the information you asked for
  + The data you get is already nicely formatted (usually JSON files)
  + You can be sure that what you do is perfectly legal and in line with ToS

- Disadvantages of API-harvesting:
  - Not every website has an API
  - You can only get what the company allows you to get
  - There are often restricting quotas (querys per day)
  - there is no standard language to make querys, you have to check the documentation
  - Not every API has a documentation
  
---

class: inverse, middle, center

# Screenscraping vs. API-harvesting

If you can, use an API, if you must, use Screenscraping instead

---

class: center, middle

# The YouTube API

---

# Summary

- Fortunately, YouTube has their own, well-documented API
  that developers can use to interact with their database (Most Google Services do)

- To find an API for a given website, [Programmable Web](https://www.programmableweb.com) is
  a good starting point

- Some prominent APIs:
  - [Google Maps](https://www.programmableweb.com/api/google-maps-places)
  - [Facebook](https://developers.facebook.com/docs/graph-api?locale=de_DE)
  - [Twitter](https://developer.twitter.com/en/docs/basics/getting-started)
  - [Instagram](https://www.instagram.com/developer/)
  - [Wikipedia](https://de.wikipedia.org/wiki/Wikipedia:Technik/Datenbank/API)

- We will use the [YouTube API](https://developers.google.com/youtube/v3/docs) today

---

# Getting started

- First of all, we need a Google account.
  - You can use an already existing one
  - you can create a new one
  - For safety reasons, we advise you to create a new account
    (If you accidentally share your login credentials, people using the credentials only
     have access to your Bot account and not your personal or work account)
  - You can create a new account [here](https://accounts.google.com/signup/v2/webcreateaccount?service=mail&continue=https%3A%2F%2Fmail.google.com%2Fmail%2F&ltmpl=googlemail&gmb=exp&biz=false&flowName=GlifWebSignIn&flowEntry=SignUp)

- Second, we need to:
  - Sign up for the Google developer console
  - Create a new Project
  - Activate the YouTube Data API
  - Create Authentification Credentials

---

# Google Developer Console

- Go to the [Developer Console](https://console.developers.google.com) and log in with your (new) Google Account

- Create a new Project by clicking on the "Create" button on the top right Corner

![plot](Images/CreateProject.png)

---

# Watch out

- You only have a limited number of Projects that you can create, so be carefull not
  to constantly delete and create new ones

- More information can be found [here](https://support.google.com/cloud/answer/6330231)

---

# Creating a New Project

- Specify a Project Name (you do not have to indicate an organisation) and click on create once you are done

![plot](Images/ProjectName.png)
---

# Activate the YouTube Data v3 API

- click on the button to "Activate APIs"

![plot](Images/ActivateAPI.png)

---

# Activate the YouTube Data and Analytics APIs

- Enter "YouTube" in the search bar, click on the data API and then activate it by clicking on the "Activate" button

![plot](Images/APIs.png)

---

![plot](Images/Activate.png)

---

![plot](Images/AuthentificationButton.png)

---

![plot](Images/UserAgreement.png)

---

![plot](Images/CreateOAuth.png)

---

![plot](Images/OtherApplication.png)

---

class: center, middle

# DONE!

---

# What can we do with these Credentials now?

- We can see what the API allows us to do in the [Google API Explorer](https://developers.google.com/apis-explorer/#p/)

- Basically, you can automate any action and see all data that you
  also would be able to see when logged into YT with your Google account

- Fortunately for us, the YouTube API is very [well documented](https://developers.google.com/youtube/v3/docs/)

---

# Test

---

# Have a slide with an overview of most common parameters and which ones we are going to use

---

# Have a slide explaining what a GET Request it, how to execute it from the terminal and

---

# how to execute it from R

---