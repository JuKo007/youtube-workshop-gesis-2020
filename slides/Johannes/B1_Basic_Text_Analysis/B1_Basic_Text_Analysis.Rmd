---
title: "Automatic sampling and analysis of YouTubeData"
subtitle: "Basic text analysis of user comments"
author: "Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni"
date: "2020-02-11"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---
layout: true

```{r setup, include = F}
if (!require(easypackages)) install.packages("easypackages")
library(easypackages)

packages("knitr",
         "rmarkdown",
         "tidyverse",
         "kableExtra",
         "hadley/emo",
         "tuber",
         "dill/emoGG",
         "ggwordcloud",
         "quanteda",
         prompt = F)

options(htmltools.dir.version = FALSE, stringsAsFactors = F)

opts_chunk$set(echo = TRUE, fig.align = "center")

```

<div class="my-footer">
  <div style="float: left;"><span>`r gsub("<br />", ", ", gsub("<br /><br />|<a.+$", "", metadata$author))`</span></div>
  <div style="float: right;"><span>`r metadata$location`, `r metadata$date`</span></div>
  <div style="text-align: center;"><span>`r gsub(".+<br />", " ", metadata$subtitle)`</span></div>
</div>

<style type="text/css">

pre {
  font-size: 10px
}
</style>

---

# Text mining

In this session, we will discuss some basic exploratory analyses of *YouTube* user comments. We will explore the use of words as well as the use of emojis.

An introduction to text mining is beyond the scope of this workshop, but there are many great introductions available (for free) online. For example:

- [Text Mining in R](https://www.tidytextmining.com/) by Julia Silge & David Robinson: A tidy(verse) approach
- [Tutorials for the package `quanteda`](https://tutorials.quanteda.io/)
- [Text mining for humanists and social scientists in R](https://tm4ss.github.io/docs/) by Andreas Niekler & Gregor Wiedemann
- [Automatisierte Inhaltsanalyse mit `R`](http://inhaltsanalyse-mit-r.de/) by Cornelius Puschmann

In the following, we will very briefly introduce some key terms and steps in text mining, and then go through some examples of exploring *YouTube* comments (text + emojis).

---

# Text as data (in a nutshell)

**Document** = collection of strings (+ metadata about the documents)

**Corpus** = collection of documents

**Token** = part of a text that is a meaningful unit of analysis (often individual words)

**Vocabulary** = list of all distinct words form a corpus

**Document-term matrix (DTM)** or **Document-feature matrix (DFM)** = matrix with *n* = # of documents rows and *m* = size of vocabulary columns where each cell contains the count of a particular word for a particular document

---

# Preprocessing (in a nutshell)

For our examples in this session, we will go through the following preprocessing steps:

1. **Basic string operations**: 
  - Transforming to lower case
  - Detecting and removing certain patterns in strings (e.g., punctuation, numbers or URLs)
2. **Tokenization**: Splitting up strings into words (could also be combinations of multiple words: n-grams)
3. **Stop word removal**: Stopwords are very frequent words that appear in almost all texts (e.g., "a","but","it", "the")

**NB**: There are many other preprocessing options that we will not use for our examples, such as [stemming](https://en.wikipedia.org/wiki/Stemming), [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) or natural language processing pipelines (e.g., to detect and select specific word types, such as nouns and adjectives). Keep in mind that the choice and order of these preprocessing steps is important and should be informed by your research question.

---

# Collect the data & parse the comments

The following code will only work after completing the necessary authentication steps.

```{r get & parse comments, eval=F}
Comments <- get_all_comments(c(video_id="DcJFdCmN98s"))
source("yt_parse.R") # for this to work, the script yt_parse.R needs to be in the same directory
FormattedComments <- yt_parse(Comments) # while take a while to complete
```

```{r load data, echo=F}
load("C:/Users/breuerjs/Nextcloud/YouTube-Workshop/Material/YouTubeComments-master/Data/ParsedCommentsUTF8.Rdata")
```

---

# Data cleaning

We will use the column including the comment text without emojis for the following examples. First, we need to remove newline commands from comment texts.

```{r comment cleaning}
FormattedComments$TextEmojiDeleted <- gsub(FormattedComments$TextEmojiDeleted,
                                           pattern = "\\\n",
                                           replacement = " ")
```

Next, we tokenize the comments and remove punctuation, symbols, numbers, and URLs.

```{r tokenization}
toks <- tokens(char_tolower(FormattedComments$TextEmojiDeleted),
               remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_separators = TRUE,
               remove_symbols = TRUE,
               remove_hyphens = TRUE,
               remove_url = TRUE)
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/tidyverse-workshop-gesis-2019/exercises/A5_DataWrangling1_exercises_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/tidyverse-workshop-gesis-2019/solutions/A5_DataWrangling1_exercises_solution.html)