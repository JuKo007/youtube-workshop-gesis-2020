<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Automatic Sampling and Analysis of YouTube Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Kohne Johannes Breuer M. Rohangis Mohseni" />
    <meta name="date" content="2020-02-10" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="../workshop.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic Sampling and Analysis of YouTube Data
## Processing and Cleaning User Comments
### Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni
### 2020-02-10

---

class: center, middle

# Processing and Cleaning User Comments

---
# Preprocessing

- Preprocessing refers to all steps that need to be taken to make the data suitable for the actual analysis

- For webscraping data, this is often more tedious and time-consuming than for survey data because:
  - the data is not designed with your analysis in mind
  - the data is typically less structured
  - the data is typically more complex
  - the data is typically more heterogenous
  - the data is typically larger
  
- In addition, it's often necessary to work on Servers instead of regular PCs

- Even then, restructuring or transforming data can take days, so mistakes hurt more

---
# Preprocessing

- In _Data Science_, most time is typically spent on the preprocessing, rather than the actual analysis

![plot](./Images/DS2.jpg)

.tiny[Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#157890a96f63]

---
# Preprocessing

- Also, it is perceived as the least enjoyable part of the process

![plot](./Images/DS1.jpg)

.tiny[Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#157890a96f63]
---
# Preprocessing _YouTube_ comments

- The `tuber` package already returns an R dataframe instead of a JSON

- We can already select which data we need by using the API through `tuber`

- For single videos, the data is small enough to be processed on a regular PC

- However, this doesn't mean that the data is already usable for all intents and purposes

- We still need to:
  - select
  - format
  - extract
  - link
  
the information that is relevant to us

---
# Preprocessing _YouTube_ comments

Loading the unprocessed comments into R


```r
# loading raw data (This is the BackUp file)
load("../../data/RawComments.Rdata")
```

---
# Understanding your data

The first step is always to understand your data, this is especially crucial for _found data_ because
it was not designed with your analysis in mind


```r
# listing all columns
colnames(comments)
```

```
##  [1] "authorDisplayName"     "authorProfileImageUrl" "authorChannelUrl"     
##  [4] "authorChannelId.value" "videoId"               "textDisplay"          
##  [7] "textOriginal"          "canRate"               "viewerRating"         
## [10] "likeCount"             "publishedAt"           "updatedAt"            
## [13] "id"                    "parentId"              "moderationStatus"
```

Luckily, the _YouTube_ API is very [well documented](https://developers.google.com/youtube/v3/docs/comments) and provides brief explanations for all the variables you can extract from it

---
# Understanding your data

This information is valuable for understanding missing data


```r
table(is.na(comments$parentId))
```

```
## 
## FALSE  TRUE 
##  1899  3878
```

A quick look into the documentation reveals:

**parentID**: _The unique ID of the parent comment. This property is only set if the comment was submitted as a reply to another comment._

---
# Understanding your data

...or for knowing how specific datatypes are formatted


```r
head(comments$publishedAt)
```

```
## [1] "2020-02-04T03:24:32.000Z" "2020-02-02T22:54:09.000Z"
## [3] "2020-02-02T21:22:39.000Z" "2020-02-02T07:21:58.000Z"
## [5] "2020-01-31T08:06:44.000Z" "2020-01-30T14:19:34.000Z"
```

```r
class(comments$publishedAt)
```

```
## [1] "character"
```

A quick look into the documentation reveals:

**publishedAt**: _The date and time when the comment was orignally published. The value is specified in ISO 8601 (YYYY-MM-DDThh:mm:ss.sZ) format._

---
# Understanding your data

...or how similar variables are different from each other


```r
comments$textOriginal[8]
```

```
## [1] "... 0:39 Is that the Crying Indian in the back?"
```

```r
comments$textDisplay[8]
```

```
## [1] "... &lt;a href=\"https://www.youtube.com/watch?v=1aheRpmurAo&amp;amp;t=0m39s\"&gt;0:39&lt;/a&gt; Is that the Crying Indian in the back?"
```

A quick look into the documentation reveals:

**textOriginal**: _The original, raw text of the comment as it was initially posted or last updated. The original text is only returned if it is accessible to the authenticated user, which is only guaranteed if the user is the comment's author._

**textDisplay**: _The comment's text. The text can be retrieved in either plain text or HTML. (The comments.list and commentThreads.list methods both support a textFormat parameter, which specifies the desired text format). Note that even the plain text may differ from the original comment text. For example, it may replace video links with video titles._

---
# Selecting what you (don't) need

After understanding the variables in our dataset, we can decide on what we need for further analysis


```r
Selection &lt;- subset(comments,select = -c(authorProfileImageUrl,
                                         authorChannelUrl,
                                         authorChannelId.value,
                                         videoId,
                                         canRate,
                                         viewerRating,
                                         moderationStatus))
colnames(Selection)
```

```
## [1] "authorDisplayName" "textDisplay"       "textOriginal"     
## [4] "likeCount"         "publishedAt"       "updatedAt"        
## [7] "id"                "parentId"
```

**Word of advice**: Always keep an unalterd copy of your raw data and don't overwrite it. You never know what kinds of mistakes/oversights you might notice down the line and you don't want to have to recollect everything. Save your parsed data in a seperate file (or in multiple steps if your preprocessing pipeline is complex).

---
# Formatting your data

By default, the data you get out of `tuber` is most likely not in the right format


```r
sapply(Selection, class)
```

```
## authorDisplayName       textDisplay      textOriginal         likeCount 
##       "character"       "character"       "character"       "character" 
##       publishedAt         updatedAt                id          parentId 
##       "character"       "character"       "character"       "character"
```

Consider for example the `likeCount` or the `publishedAt` timestamp


```r
# Summary statistics for like counts
summary(Selection$likeCount)
```

```
##    Length     Class      Mode 
##      5777 character character
```


```r
# time difference between first comment and now
Sys.time() - Selection$publishedAt[1]
```

```
## Error in unclass(e1) - e2: non-numeric argument to binary operator
```

---
# Formatting `likeCount`

We want the `likeCount` to be a numeric variable and the timestamps to be datetime objects


```r
# Transforming likeCount to numeric (carefull, this is overwriting the column)
Selection$likeCount &lt;- as.numeric(Selection$likeCount)

# testing
summary(Selection$likeCount)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    0.00    0.00   12.29    1.00 3903.00
```

We can now work with the number of likes as a numeric variable

---
# Formatting your timestamps

Timestamps are extremely complex objects due to:
 - Different calendars
 - Different formattings
 - Different origins
 - Different time zones
 - Historical anomalies
 - Different resolutions
 - Summer vs. Wintertime (different for each country and depending on hemisphere!)
 - Leap years
 - [etc.](https://www.youtube.com/watch?v=-5wpm-gesOY)
 
For these reasons, **never** try to code your own timestamp translations from scratch. Fortunately, R has several build in methods to deal with this madness. The most basic one as the `as.POSIXct()` function, the most convenient one is the `anytime()` function from the `anytime` package.

---
# Formatting timestamps


```r
# transforming timestamps to datetime objects
Selection$publishedAt[1]
```

```
## [1] "2020-02-04T03:24:32.000Z"
```

```r
testtime &lt;- as.POSIXct(Selection$publishedAt[1], format = "%Y-%m-%dT%H:%M:%OSZ", tz = "UTC"); testtime
```

```
## [1] "2020-02-04 03:24:32 UTC"
```

```r
# testing whether we can compute a difference with the datetime object
Sys.time() - testtime
```

```
## Time difference of 1.648414 days
```

This internal representation of time objects will be extremely important for plotting trends over time
and calculating time differences

---
# Formatting timestamps

A more convenient way to transform datetimes is the `anytime` package. Basically, it automatically tries
to guess the format from the cahracter string, so you don't have to. This is especially handy for vectors 
of datetimes in multiple different formats.


```r
# transforming datetimes using anytime()
library(anytime)
Selection$publishedAt &lt;- anytime(Selection$publishedAt, asUTC = TRUE)
class(Selection$publishedAt )
```

```
## [1] "POSIXct" "POSIXt"
```

```r
Selection$updatedAt &lt;- anytime(Selection$updatedAt, asUTC = TRUE)
class(Selection$updatedAt )
```

```
## [1] "POSIXct" "POSIXt"
```

**Word of Advice**: For datetime conversions, always do some sanity spotchecks, especially when you are using methods that automatically detect the format. Give special attention to the _timezone_ in which your data is saved and compare it to the documentation of the standard.

---
# Formatting timestamps

Be aware of how to interpret your timestamps. Note that the date was interpreted as UTC but converted to our local CET timezone which is 1 hour ahead of UTC. This comment was made at 04:24:32 in _our time_, we have no idea about the time
at the location of the user. She might of made this comment at night or in the morning, depending on where she's from.


```r
Selection$publishedAt[1]
```

```
## [1] "2020-02-04 04:24:32 CET"
```
 
---
# Extracting information

After having formatted all our selected columns, we usually also want to create some new ones with information
that is not directly available in the raw data. Consider for example our these comments:


```r
# Example comments with extractable information
Selection$textOriginal[39]
```

```
## [1] "More bullshit 😷🤢"
```

```r
Selection$textOriginal[495]
```

```
## [1] "Sure, here you go: https://allthatsinteresting.com/iron-eyes-cody"
```

There are two issues exemplefied by these comments:

1) Comments contain emojis and hyperlinks that might distort our text analysis later

2) These are features that we'd like to have in a seperate column for our analysis

---
# Extracting hyperlinks

We will start with deleting hyperlinks from our text and saving them in an additional column. We will use the
textmining package `qdapRegex` for this, that has predefined routines for handling large textvectors and regular
expressions. You can learn more about regular expressions [here](https://en.wikipedia.org/wiki/Regular_expression).


```r
# Note that we are using the original text so we don't have to deal with the HTML formatting of the links
library(qdapRegex)
Links &lt;- rm_url(Selection$textOriginal, extract = TRUE)
textNew &lt;- rm_url(Selection$textOriginal)
head(Links,3)
```

```
## [[1]]
## [1] NA
## 
## [[2]]
## [1] "https://youtu.be/GWCySrYxov0"
## 
## [[3]]
## [1] NA
```

---
# Extracting hyperlinks

We get back a list where each element corresponds to one row in the Selection dataframe and contains a vector of
links that were contained in the textOriginal column. At the same time, the link was removed from the Selection dataframe.


```r
textNew[495]
```

```
## [1] "Sure, here you go:"
```

```r
Links[495]
```

```
## [[1]]
## [1] "https://allthatsinteresting.com/iron-eyes-cody"
```

---
# Extracting emoji

The `qdapRegex` package has a lot of other different predefined functions to extract or remove certain kinds of strings:
  - `rm_citation()`
  - `rm_date()`
  - `rm_phone()`
  - `rm_postal_code()`
  - `rm_email()`
  - `rm_dollar()`
  - `rm_emoticon()`
  
Unfortunately, it does **not** contain a predefined method for emoji, so we will have to use the `emo` package for
removing the emoji and come up with our own method for extracting them.

---
# Extracting emoji

First of all, we remove the emoji from our `textNew` variable to have one _clean_ column that we can use for textmining later


```r
# We take the textNew column and also delete the emoji from it
library(emo)
textNew[39]
```

```
## [1] "More bullshit 😷🤢"
```

```r
textNew &lt;- ji_replace_all(textNew,"")
textNew[39]
```

```
## [1] "More bullshit "
```

---
# Extracting emoji

Next, we want to replace the emoji with a textual description, so that we can treat it just like any other token in text mining. This is no trivial task, as we have to go through each comment and replace each emoji with it's respective textual description. Unfortuntely, we did not find a working, easy to use, out of the box solution for this. But we can always make our own!

Essentially, we want to replace this:


```r
emo::ji("monkey")
```

```
## 🐒
```

with this


```r
"EMOJI_Monkey"
```

```
## [1] "EMOJI_Monkey"
```

---
# Extracting Emoji

First of all, we need a dataframe that contains the emojis as they are internally represented by R (this can be quite the [hassle](https://dss.iq.harvard.edu/blog/escaping-character-encoding-hell-r-windows). Luckily, this is contained in the `emo` package


```r
library(emo)
EmojiList &lt;- jis
head(EmojiList,3)
```

```
##   runes       qualified emoji                           name            group
## 1 1F600 fully-qualified     😀                  grinning face Smileys &amp; People
## 2 1F601 fully-qualified     😁 beaming face with smiling eyes Smileys &amp; People
## 3 1F602 fully-qualified     😂         face with tears of joy Smileys &amp; People
##        subgroup version points nrunes vendor_apple vendor_google vendor_twitter
## 1 face-positive     6.1 128512      1         TRUE          TRUE           TRUE
## 2 face-positive     6.0 128513      1         TRUE          TRUE           TRUE
## 3 face-positive     6.0 128514      1         TRUE          TRUE           TRUE
##   vendor_one vendor_facebook vendor_messenger vendor_samsung vendor_windows
## 1       TRUE            TRUE             TRUE           TRUE           TRUE
## 2       TRUE            TRUE             TRUE           TRUE           TRUE
## 3       TRUE            TRUE             TRUE           TRUE           TRUE
##                                                 keywords
## 1                face, grin, grinning face, smile, happy
## 2 beaming face with smiling eyes, eye, face, grin, smile
## 3  face, face with tears of joy, joy, laugh, tear, tears
##                                aliases unicode_version ios_version
## 1              grinning, grinning_face             6.1           6
## 2 grin, beaming_face_with_smiling_eyes             6.0           6
## 3          joy, face_with_tears_of_joy             6.0           6
```

---
# Extracting Emoji

Next, we need to paste the names of the Emoji together while capitalizing every words first letter for better readibility


```r
# Defining a function for capitalizing and pasting names together
simpleCap &lt;- function(x) {

  # Splitting the string
  splitted &lt;- strsplit(x, " ")[[1]]

  # Pasting it back together with capital letters
  paste(toupper(substring(splitted, 1,1)), substring(splitted, 2),sep = "", collapse = " ")

}
```



```r
# Applying the function to all the names
CamelCaseEmojis &lt;- lapply(jis$name, simpleCap)
CollapsedEmojis &lt;- lapply(CamelCaseEmojis, function(x){gsub(" ", "", x, fixed = TRUE)})
EmojiList[,4] &lt;- unlist(CollapsedEmojis)
head(EmojiList[,4],3)
```

```
## [1] "GrinningFace"               "BeamingFaceWithSmilingEyes"
## [3] "FaceWithTearsOfJoy"
```


---
# Extracting Emoji

Next, we need to order our dictionary from longest to shortest, so that we can prevent partial matching of shorter strings later


```r
EmojiList &lt;- EmojiList[rev(order(nchar(jis$emoji))),]
head(EmojiList[,c(1,3,4)],5)
```

```
##                                           runes emoji             name
## 1862 1F469 200D 2764 FE0F 200D 1F48B 200D 1F469  👩‍❤️‍💋‍👩 Kiss:Woman,Woman
## 1860 1F468 200D 2764 FE0F 200D 1F48B 200D 1F468  👨‍❤️‍💋‍👨     Kiss:Man,Man
## 1858 1F469 200D 2764 FE0F 200D 1F48B 200D 1F468  👩‍❤️‍💋‍👨   Kiss:Woman,Man
## 3570  1F3F4 E0067 E0062 E0077 E006C E0073 E007F     🏴󠁧󠁢󠁷󠁬󠁳󠁿            Wales
## 3569  1F3F4 E0067 E0062 E0073 E0063 E0074 E007F     🏴󠁧󠁢󠁳󠁣󠁴󠁿         Scotland
```
Note that what we are ordering by is the `emoji` column, not the `text` or `runes` columns

---
# Extracting Emoji

Next, we need to `loop` through all of our emojis and replace them one after the other in each comment (this may take a while)


```r
# Assigning the column to a new variable so we can overwrite it iteratively
New &lt;- Selection$textOriginal

# Looping through all Emojis for all comments in textNew
for (i in 1:dim(EmojiList)[1]) {

  New &lt;- rm_default(New,
                    pattern = EmojiList[i,3],
                    replacement = paste0("EMOJI_", EmojiList[i,4], " "),
                    fixed = TRUE,
                    clean = FALSE,
                    trim = FALSE)
}
```

As output, we get a large character vector where all the contained emojis are replaced with their respective textual descriptions. This makes the comment more human readable on systems that can't display emojis


```r
New[39]
```

```
## [1] "More bullshit EMOJI_FaceWithMedicalMask EMOJI_NauseatedFace "
```

---
# Extracting Emoji

We now have a vector of comments with replaced emoji, but we still want to extract them into their own vector


```r
ExtractEmoji &lt;- function(x){

  SpacerInsert &lt;- gsub(" ","[{[SpAC0R]}]", x)
  ExtractEmoji &lt;- rm_between(SpacerInsert,
                             "EMOJI_","[{[SpAC0R]}]",
                             fixed = TRUE,
                             extract = TRUE,
                             clean = FALSE,
                             trim = FALSE,
                             include.markers = TRUE)
  
  UnlistEmoji &lt;- unlist(ExtractEmoji)
  DeleteSpacer &lt;- sapply(UnlistEmoji,
                         function(x){gsub("[{[SpAC0R]}]"," ",x,fixed = TRUE)})
  
  names(DeleteSpacer) &lt;- NULL

  Emoji &lt;- paste0(DeleteSpacer, collapse = "")
  
  return(Emoji)
}
```
---
# Extracting Emoji


```r
Emoji &lt;- sapply(New,ExtractEmoji)
Emoji[39]
```

```
## More bullshit EMOJI_FaceWithMedicalMask EMOJI_NauseatedFace  
##             "EMOJI_FaceWithMedicalMask EMOJI_NauseatedFace "
```

---
# Extracting information

We now have different versions of our text column

1) The original one, with hyperlinks and Emoji

2) One with only plain text and without hyperlinks and emoji

3) One with only hyperlinks

4) One with only emoji

We want to integrate them in our dataframe

---
# Linking everything back together

We can now recombine our dataframe with the additional columns we created to have the perfect starting point for our analysis! However, because we sometimes have more than two links or two emojis per comment, we need to use the `I()` function so we can put them in the dataframe `as is`. Later, we will have to unlist these columns rowwise if we want to use them. 


```r
Selection &lt;- cbind.data.frame(Selection,I(Emoji),New,I(Links), stringsAsFactors = FALSE)
```

At last, we can save the dataframe for later use


```r
save(Selection, file = "ParsedComments.Rdata")
```

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
