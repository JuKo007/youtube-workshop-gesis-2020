---
title: 'Basic text analysis of user comments - Solutions'
# title: 'Basic text analysis of user comments' 
author: 'Julian Kohne, Johannes Breuer, M. Rohangis Mohseni'
date: 'Automatic sampling and analysis of YouTube data, February 10-11, 2020'
output:
  unilur::tutorial_html_solution: default
  # unilur::tutorial_html: default
---

```{r knitr_init, echo=FALSE, cache=FALSE, include=FALSE}
# custom boxes
knitr::opts_template$set(clues = list(box.title = "Clues",
                                      box.body = list(fill = "#fff9dc", colour = "black"),
                                      box.header = list(fill = "#ffec8b", colour = "black"),
                                      box.icon = "fa-search",
                                      box.collapse = TRUE))

```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

In the following exercises, we will use the data you have collected in the previous session (all comments for the video "The Census" by Last Week Tonight with John Oliver. You might have to adjust the following code to use the correct file path on your computer.

```{r load data}
comments <- readRDS("../data/LWT_Census_parsed.rds")
```

Next, we go through the preprocessing steps described in the slides. In a first step, we remove newline commands from the comment strings (without emojis).

```{r remove newline, message = F}
library(tidyverse)

comments <- comments %>% 
  mutate(TextEmojiDeleted = str_replace_all(TextEmojiDeleted,
                                            pattern = "\\\n",
                                            replacement = " "))
```

Next, we tokenize the comments and create a document-feature matrix from which we remove English stopwords.

```{r tokens & DFM, message = F}
library(quanteda)

toks <- comments %>% 
  pull(TextEmojiDeleted) %>% 
  char_tolower() %>% 
  tokens(remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_separators = TRUE,
               remove_symbols = TRUE,
               remove_hyphens = TRUE,
               remove_url = TRUE)

comments_dfm <- dfm(toks, 
                   remove = quanteda::stopwords("english"))
```


```{block, box.title = "1", box.body = list(fill = "white"), box.icon = "fa-star"}
Which are the 20 most frequently used words in the comments on the video "The Census" by Last Week Tonight with John Oliver? Save the overall word ranking in a new object called `term_freq`.
```

```{block, opts.label = "clues"}
You can use the function `textstat_frequency()` from the `quanteda` package to answer this question.
```

```{r term freq, solution = TRUE}
term_freq <- textstat_frequency(comments_dfm)
head(term_freq, 20)
```
